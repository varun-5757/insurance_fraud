{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SageMaker Linear Learner - for Binary Classification\n",
    "\n",
    "\n",
    "SageMaker allows us to create and train a model in a managed environment, inside containers. Therefore, you'll notice that in this stage when we move on to productionize our model pipeline, we do not use the Machine Learning framework inside our Notebook.\n",
    "\n",
    "Inside the SageMaker managed containers, the algorithms, whether you choose the built-in algorithms, or create a container with your own algoritm, run in their own execution environment. In case of SageMaker built-in algorithms, they are implemented using MXNet as framework of choice. If you build your own algorithm, you are free to choose any framework you prefer.\n",
    "\n",
    "In either case, you can then invoke the containers to trigger training and deployment of models using one of 3 ways:\n",
    "- Using AWS SageMaker Console GUI\n",
    "- Using AWS CLI\n",
    "- Using Language specific API (in this case Python - Boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuring an S3 bucket to store training and validation data, and capture model artifacts\n",
    "\n",
    "Before we build, train and deploy our model we need a place to store the training and validation data and capture the model artifacts.  In this case, since we are using Python, notice below that we're using the [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html?id=docs_gateway) interface to communicate with SageMaker API, and S3 to store the data and model Artifacts.  Hence there are no other ML specific libraries imported in this notebook. \n",
    "\n",
    "The code below connects to an S3 bucket, creates a root folder in that bucket and a subfolder entitled data.  This is where the training and validation files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be staged at s3://sagemaker-us-east-1-383064807500/insurance_fraud_prediction/data\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "#Create a SageMaker session, you'd use this to use an S3 bucket that SageMaker automatically associates for your use\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Get hold of the region you are operating, you'd use this to get ARN for algorithm container for that region\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "#Get the session default bucket\n",
    "s3_bucket = session.default_bucket()\n",
    "\n",
    "#Designate location in S3 bucket where data will be staged\n",
    "s3_root = 'insurance_fraud_prediction'\n",
    "s3_data_prefix = s3_root + '/data'\n",
    "print(\"Data will be staged at s3://{}/{}\".format(s3_bucket, s3_data_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Staging, cleaning and preparing data\n",
    "\n",
    "Since we already conducted data exploration using Scikit-Learn locally on our notebook, we now need to stage the data to be used in SageMaker managed training. \n",
    "\n",
    "When using SageMaker Managed training, note in the diagram below, that unlike our previous experiment, the training would not run on the Jupyter Notebook in itself. Instead, you would be supplying cleaned up data, as stage on an S3 bucket, pointing to a specific algorithm container, and SageMaker will manage procuring the training instance, running the training (in distributed mode, if chosen), de-provisioning of the traingn infrastructure, and persisiting the training output (in form of trained model file(s)) on S3 bucket.\n",
    "\n",
    "![Image](https://miro.medium.com/max/986/1*gpgXxd6uEF22ugNWswidNw.png)\n",
    "\n",
    "\n",
    "Staging the data on an S3 buclet is therefore the first step in starting a training job.\n",
    "\n",
    "Since we didn't save the processed data frame during exploratory phase within Scikit-Learn, we'll grab the raw data and perform following steps:\n",
    "\n",
    "- Remove unnecessary columns - during the feature engineering portion of the project, we determined that policy_number, policy_bind_date, insured_zip, incident_location, and incident_date were extraneous fields and did not help us to solve the ML problem. So these will be removed.\n",
    "- Arrange columns in order, as required\n",
    "- Encode columns to numeric values - Here we are taking the Yes (Y) and No (N) values for the field fraud_reported and converting these to 1 (Yes) and 0 (No). This will allow us to do math on these values later.\n",
    "- Split into training and validation sets - Validation data set is not used during the actual training, and is used to test the performance of the model. In the cell below we randomly select 80% of data as training_data set and 20% as val_data set.  \n",
    "\n",
    "\n",
    "Once the cell is processed you will see the results of building these two data sets with the number of features and records per set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 162 Features, 800 Records\n",
      "Validation Data : 162 Features, 200 Records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load raw data from source\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/varun-5757/insurance_fraud/main/insurance_claims.csv\").drop(columns=\"_c39\")\n",
    "\n",
    "#Remove unnecessary columns\n",
    "colsToDelete = [\"policy_number\", \"policy_bind_date\", \"insured_zip\", \"incident_location\", \"incident_date\"]\n",
    "for col in colsToDelete:\n",
    "    del data[col]\n",
    "    \n",
    "#Switch target column beginning of the data frame, as is required by SageMaker Linear Learner algorithm\n",
    "cols = list(data.columns)\n",
    "cols = [cols[-1]]+cols[:-1]\n",
    "data = data[cols]\n",
    "\n",
    "#Change target columns to have '0' and '1' values only\n",
    "data['fraud_reported'] = data['fraud_reported'].apply(lambda x: 1 if x=='Y' else 0)\n",
    "\n",
    "#Encode all other categorical column using one-hot encoding\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "#Split transformed dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=30)    \n",
    "\n",
    "print(\"Training Data : {} Features, {} Records\".format(train_data.shape[1], train_data.shape[0]))\n",
    "print(\"Validation Data : {} Features, {} Records\".format(val_data.shape[1], val_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pushing the traning and validation dataset to S3\n",
    "\n",
    "During training, SageMaker can use data either from an [S3](https://aws.amazon.com/s3/) bucket or from [Amazon Elastic File System (EFS)](https://aws.amazon.com/efs/). \n",
    "\n",
    "\n",
    "In this case we'll use S3 for data staging. So that after pre-processing, the training and validation datasets are uploaded into S3 bucket.\n",
    "\n",
    "The Location of data files on S3 bucket will be used as training and validation channels while setting up the predictor.\n",
    "\n",
    "Once the code in the cell below is finished processing, there are three results:\n",
    "\n",
    "- The training data set is saved as a csv file (train_data.csv) and placed in the data subfolder for the notebook.\n",
    "- The validation data set is saved as a csv file (val_data.csv) and placed in the data subfolder for the notebook.\n",
    "- A new subfoder is created for the model artififacts from the notebook called output.  This is where the models created from the notebook are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to: s3://sagemaker-us-east-1-383064807500/insurance_fraud_prediction/data/train_data.csv\n",
      "Validation data uploaded to: s3://sagemaker-us-east-1-383064807500/insurance_fraud_prediction/data/val_data.csv\n",
      "Model artifacts will be uploaded to: s3://sagemaker-us-east-1-383064807500/insurance_fraud_prediction/output\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "#Upload training data file to S3\n",
    "csv_buffer = io.StringIO()\n",
    "train_data.to_csv(csv_buffer, header=False, index=False)\n",
    "s3_train_data_key = '{}/train_data.csv'.format(s3_data_prefix)\n",
    "s3_resource.Object(s3_bucket, s3_train_data_key).put(Body=csv_buffer.getvalue())\n",
    "s3_train_data = 's3://{}/{}'.format(s3_bucket, s3_train_data_key)\n",
    "print(\"Training data uploaded to: {}\".format(s3_train_data))\n",
    "\n",
    "#Upload validation data file to S3\n",
    "csv_buffer = io.StringIO()\n",
    "val_data.to_csv(csv_buffer, header=False, index=False)\n",
    "s3_val_data_key = '{}/val_data.csv'.format(s3_data_prefix)\n",
    "s3_resource.Object(s3_bucket, s3_val_data_key).put(Body=csv_buffer.getvalue())\n",
    "s3_val_data = 's3://{}/{}'.format(s3_bucket, s3_val_data_key)\n",
    "print(\"Validation data uploaded to: {}\".format(s3_val_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(s3_bucket, s3_root)\n",
    "print('Model artifacts will be uploaded to: {}'.format(output_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Selection\n",
    "\n",
    "Since our initial experiment showed promising results on this dataset using logistic regression within Scikit-Learn, we can implement the same using SageMaker using Linear Learner model with Binary classifier predictor type. \n",
    "\n",
    "SageMaker's linear-learner algorithm performs logistic regression assuming a linear separation boundary between two classes.\n",
    "\n",
    "![Image](https://miro.medium.com/max/1680/0*gKOV65tvGfY8SMem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtaining the Docker container image for the linear-learner algorithm and generating unique names for model artifacts\n",
    "\n",
    "First step therefore is to get hold of the container image for `linear-learner` in the region we are operating.  \n",
    "\n",
    "As a first step, we now need to obtain the container image for the `linear-learner` algorithm in the region we are operating in.  \n",
    "\n",
    "Each training job also needs a unique name, which we create by appending current timestamp with a name of our choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm container: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Obtain container image for Linear Learner Algorithm\n",
    "#container = get_image_uri(region, 'linear-learner')\n",
    "\n",
    "image_uris.retrieve(region=boto3.Session().region_name, framework='linear-learner', version='1')\n",
    "print(\"Algorithm container: {}\".format(container))\n",
    "\n",
    "# Obtain SageMaker execution role that will be used to invoke Sagemaker commands\n",
    "role = get_execution_role()\n",
    "\n",
    "# SageMaker client using Python boto3 API\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "#Generate unique names for artefacts, with timestamp, for easy tracking\n",
    "pipeline_name = 'insurance-fraud-predictor'\n",
    "runtime = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n",
    "run_name = \"{}-{}\".format(pipeline_name, runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model training\n",
    "\n",
    "With the image URI readily available, not a single line of Model definition or training code needs to be written.\n",
    "Instead we could follow one of two simple approaches:\n",
    "    \n",
    "1. Create an instance of SageMaker Estimator, which provides a familar programming interface common to most other Machine Language frameworks, such as Scikit-Learn, and then invoke a `fit` operation. This is scikit-learn emulation.\n",
    "\n",
    "2. Use `boto3` API to create a sagemaer training job, supplying the training image, hyperparameters, input and output data and resource configurations.\n",
    "\n",
    "Both approaches produce same result, which is to train a model on a specified dataset using the specified algorithm container, and save the model artifact to an S3 bucket.\n",
    "\n",
    "In this case, we'll use the later (Example using the former approach can be found [here](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb))  See the code cell 7 and 8 in this notbook for details on how to use the SageMaker Estimator, including the fit operation.\n",
    "\n",
    "**The cell below addresses the following objectives from the Algorithms Module:**\n",
    "\n",
    "- **Configure and initiate a training job**\n",
    "- **Associate the training job with the appropriate S3 bucket**\n",
    "- **Select the desired or default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job - arn:aws:sagemaker:us-east-1:383064807500:training-job/insurance-fraud-predictor-2021-03-02-15-04-37-940 started\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker.create_training_job(\n",
    "    TrainingJobName=run_name,\n",
    "    HyperParameters={\n",
    "        'feature_dim': str(data.shape[1]-1),\n",
    "        'predictor_type': 'binary_classifier',\n",
    "        'binary_classifier_model_selection_criteria': 'f_beta',\n",
    "        'optimizer': 'sgd',\n",
    "        'loss': 'logistic',\n",
    "        'normalize_data': 'auto',\n",
    "        'epochs': '20',\n",
    "        'mini_batch_size': '200'\n",
    "    },\n",
    "    AlgorithmSpecification={ 'TrainingImage': container, 'TrainingInputMode': 'File'},    \n",
    "    RoleArn=role,\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'ContentType': 'text/csv',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {'S3DataType': 'S3Prefix','S3Uri': s3_train_data,'S3DataDistributionType': 'FullyReplicated'}\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'validation',\n",
    "            'ContentType': 'text/csv',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': s3_val_data,\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None'\n",
    "        },        \n",
    "    ],\n",
    "    OutputDataConfig={'S3OutputPath': output_location},\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.m4.xlarge',\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 10\n",
    "    },\n",
    "    StoppingCondition={'MaxRuntimeInSeconds': 86400},\n",
    "    Tags=[{\n",
    "            'Key': 'Name',\n",
    "            'Value': '{}-training'.format(run_name)\n",
    "        }]    \n",
    ")\n",
    "\n",
    "print(\"Training Job - {} started\".format(response['TrainingJobArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this particular example we are not using any Hyperparameter optimization, instead we are using some hyperparameters that work best on this dataset, based on our previous experiment.\n",
    "\n",
    "Where manually deciding the right set of hyper-parameters is difficult, and the default set of hyparameters do not produce required result, we can use [Automated Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) feature of SageMaker to arrive at an optimal set fo Hyperparameters.  \n",
    "\n",
    "\n",
    "Depending upon the complexity of the model and size of training datset, it will take few minutes to few hours for the model to be ready. \n",
    "\n",
    "We therefore wait until the model get trained. In this case, since we are using a very small datset, the training should not be more than 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job insurance-fraud-predictor-2021-03-02-15-04-37-940 - InProgress - Time Elapsed: 0 seconds\n",
      "Training job insurance-fraud-predictor-2021-03-02-15-04-37-940 - Completed - Time Elapsed: 10 seconds\n"
     ]
    }
   ],
   "source": [
    "status='InProgress'\n",
    "step = 0\n",
    "sleep = 10\n",
    "print(\"Training job {} - {} - Time Elapsed: {} seconds\".format(run_name, status,step*sleep))\n",
    "while status != 'Completed' and status != 'Failed':\n",
    "    response = sagemaker.describe_training_job(\n",
    "        TrainingJobName=run_name\n",
    "    )\n",
    "    status = response['TrainingJobStatus']\n",
    "    time.sleep(sleep)\n",
    "    step = step+1\n",
    "    print(\"Training job {} - {} - Time Elapsed: {} seconds\".format(run_name, status,step*sleep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Hosting and Inference\n",
    "\n",
    "Now that our model is trained, we can now deploy it behind an HTTP endpoint.\n",
    "\n",
    "Take a look a the graphic below.  The graphic illustrates model deployment with SageMaker Hosting Services with an HTTPs endpoint where your machine learning model is able to provide inferences.\n",
    "\n",
    "![Image](https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png)\n",
    "\n",
    "There are three steps involved in having a trained model deployed:\n",
    "\n",
    "1. Creating a `Model` definition that acts as a placeholder, specifying location of a model artifact (on Amazon S3 or Amazon Elastic File System) and ECS registry path of inference code image.\n",
    "\n",
    "\n",
    "2. Creating an `Endpoint Configuration`, using the `Model` definition above that defines configurations such as number and types of instances to be used, fraction of traffic to be handled by the endpoint (once deployed), and specifies whether or not [Elastic inference](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html) (a feature that allows you to speed up throughput and decrease latency of getting real-time inferences) is to be used.\n",
    "\n",
    "\n",
    "3. Creating an `Endpoint`, using the `Endpoint Configuration` defined above, which creates a SageMaker managed container with the inference code served by a Flask web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similar to the model training step, in this example, we'll use `Boto3` API to execute the 3 steps listed above.\n",
    "\n",
    "The first step creates the `Model` definition. Once the code in the cell below is successfully processed, the model definition ARN (Amazon Resource Name) will be created and displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model definition - arn:aws:sagemaker:us-east-1:383064807500:model/insurance-fraud-predictor-2021-03-02-15-24-55-045 created\n"
     ]
    }
   ],
   "source": [
    "if status == 'Completed':\n",
    "    runtime = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n",
    "    model_name = \"{}-{}\".format(pipeline_name, runtime)    \n",
    "    response = sagemaker.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "            'Image': container,\n",
    "            'ModelDataUrl': \"{}/{}/output/model.tar.gz\".format(output_location, run_name),\n",
    "            'Environment': {\n",
    "                'string': 'string'\n",
    "            }\n",
    "        },\n",
    "        ExecutionRoleArn=role,\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'Name',\n",
    "                'Value': model_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(\"Model definition - {} created\".format(response['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Second step creates the `Endpoint Configuration`.  Once the code in the cell below is successfully processed, the endpoint configuration ARN (Amazon Resource Name) will be created and displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration - arn:aws:sagemaker:us-east-1:383064807500:endpoint-config/insurance-fraud-predictor-2021-03-02-15-25-35-101 specified\n"
     ]
    }
   ],
   "source": [
    "runtime = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n",
    "endpoint_config_name = \"{}-{}\".format(pipeline_name, runtime) \n",
    "\n",
    "response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': \"ml.t2.medium\",\n",
    "            'InitialVariantWeight': 1\n",
    "        },\n",
    "    ],\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'Name',\n",
    "            'Value': endpoint_config_name\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Endpoint configuration - {} specified\".format(response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally in the third step of deployment we deploy the `Model` behind an `Endpoint`, using the `Endpoint Configuration`.\n",
    "\n",
    "Behind the scenes SageMaker provisions instance(s) of desired type, pulls the model artifact, and configures the runtime, which takes 11 to 12 minutes.  You will see the status of the endpoint creation process updated every 30 seconds beneath the cell.  Once the endpoint creation process is complete the status will change to InService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint - arn:aws:sagemaker:us-east-1:383064807500:endpoint/insurance-fraud-predictor-2021-03-02-15-32-13-384 deployment started\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 0 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 30 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 60 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 90 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 120 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 150 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 180 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 210 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 240 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 270 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 300 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 330 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 360 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 390 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 420 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 450 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 480 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 510 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 540 seconds\n",
      "Creating Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 570 seconds\n",
      "InService Enpoint insurance-fraud-predictor-2021-03-02-15-32-13-384 - Time Elapsed: 600 seconds\n"
     ]
    }
   ],
   "source": [
    "runtime = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n",
    "endpoint_name = \"{}-{}\".format(pipeline_name, runtime) \n",
    "\n",
    "response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'string',\n",
    "            'Value': endpoint_name\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint - {} deployment started\".format(response['EndpointArn']))\n",
    "\n",
    "status='Creating'\n",
    "step = 0\n",
    "sleep = 30\n",
    "print(\"{} Enpoint {} - Time Elapsed: {} seconds\".format(status,endpoint_name,step*sleep))\n",
    "while status != 'InService' and status != 'Failed' and status != 'OutOfService':\n",
    "    response = sagemaker.describe_endpoint(\n",
    "        EndpointName=endpoint_name\n",
    "    )\n",
    "    status = response['EndpointStatus']\n",
    "    time.sleep(sleep)\n",
    "    step = step+1\n",
    "    print(\"{} Enpoint {} - Time Elapsed: {} seconds\".format(status,endpoint_name,step*sleep))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Testing\n",
    "\n",
    "Having our linear logistic regression model deployed essentially completes the deployment cycle, and we are now ready to test the accuracy of the model.\n",
    "\n",
    "\n",
    "To do so, we'll use the validation data that was saved separately in S3.  Once the code in the cell is processed, the validation data set will be loaded into this notebook as indicated by the message \"200 Test record loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 Test records loaded.\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "import json\n",
    "test_data = pd.read_csv(s3_val_data, header=None)\n",
    "print(\"{} Test records loaded.\".format(test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With each record in the validation data set, we conduct the following steps:\n",
    "\n",
    "1. Create a data dump without the target column\n",
    "\n",
    "\n",
    "2. Transform it into an input record, following the format expected by the corresponding inference code.\n",
    "\n",
    "\n",
    "3. Invoke the deployed endpoint using SageMaker Runtime client.\n",
    "\n",
    "\n",
    "4. Compare the predicted label returned with the expected label, as specified in the target column.\n",
    "\n",
    "\n",
    "5. Use the comparison result to appropriately classify the out come as one of: \n",
    "    - True Negative\n",
    "    - False Positive\n",
    "    - True Positive\n",
    "    - False Negative\n",
    "    \n",
    "Once the processing of the cell below is complete you will see a list of each record with the target and predicted values.  Where 1 indicates fraud and 0 indicates no fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-1: Target - 0, Predicted - 0\n",
      "Test-2: Target - 0, Predicted - 0\n",
      "Test-3: Target - 0, Predicted - 0\n",
      "Test-4: Target - 0, Predicted - 0\n",
      "Test-5: Target - 0, Predicted - 1\n",
      "Test-6: Target - 0, Predicted - 0\n",
      "Test-7: Target - 0, Predicted - 1\n",
      "Test-8: Target - 0, Predicted - 0\n",
      "Test-9: Target - 0, Predicted - 0\n",
      "Test-10: Target - 0, Predicted - 0\n",
      "Test-11: Target - 1, Predicted - 0\n",
      "Test-12: Target - 0, Predicted - 0\n",
      "Test-13: Target - 0, Predicted - 0\n",
      "Test-14: Target - 0, Predicted - 1\n",
      "Test-15: Target - 0, Predicted - 0\n",
      "Test-16: Target - 1, Predicted - 1\n",
      "Test-17: Target - 1, Predicted - 1\n",
      "Test-18: Target - 0, Predicted - 0\n",
      "Test-19: Target - 0, Predicted - 0\n",
      "Test-20: Target - 1, Predicted - 1\n",
      "Test-21: Target - 0, Predicted - 1\n",
      "Test-22: Target - 1, Predicted - 1\n",
      "Test-23: Target - 1, Predicted - 0\n",
      "Test-24: Target - 0, Predicted - 1\n",
      "Test-25: Target - 0, Predicted - 0\n",
      "Test-26: Target - 0, Predicted - 0\n",
      "Test-27: Target - 0, Predicted - 0\n",
      "Test-28: Target - 1, Predicted - 0\n",
      "Test-29: Target - 0, Predicted - 0\n",
      "Test-30: Target - 0, Predicted - 0\n",
      "Test-31: Target - 1, Predicted - 1\n",
      "Test-32: Target - 1, Predicted - 1\n",
      "Test-33: Target - 1, Predicted - 1\n",
      "Test-34: Target - 0, Predicted - 1\n",
      "Test-35: Target - 0, Predicted - 0\n",
      "Test-36: Target - 0, Predicted - 0\n",
      "Test-37: Target - 0, Predicted - 0\n",
      "Test-38: Target - 1, Predicted - 1\n",
      "Test-39: Target - 0, Predicted - 0\n",
      "Test-40: Target - 0, Predicted - 0\n",
      "Test-41: Target - 0, Predicted - 0\n",
      "Test-42: Target - 0, Predicted - 0\n",
      "Test-43: Target - 0, Predicted - 1\n",
      "Test-44: Target - 0, Predicted - 0\n",
      "Test-45: Target - 0, Predicted - 1\n",
      "Test-46: Target - 0, Predicted - 1\n",
      "Test-47: Target - 0, Predicted - 0\n",
      "Test-48: Target - 0, Predicted - 0\n",
      "Test-49: Target - 0, Predicted - 0\n",
      "Test-50: Target - 0, Predicted - 1\n",
      "Test-51: Target - 0, Predicted - 0\n",
      "Test-52: Target - 0, Predicted - 0\n",
      "Test-53: Target - 0, Predicted - 0\n",
      "Test-54: Target - 0, Predicted - 1\n",
      "Test-55: Target - 0, Predicted - 0\n",
      "Test-56: Target - 1, Predicted - 0\n",
      "Test-57: Target - 0, Predicted - 0\n",
      "Test-58: Target - 0, Predicted - 1\n",
      "Test-59: Target - 1, Predicted - 1\n",
      "Test-60: Target - 1, Predicted - 1\n",
      "Test-61: Target - 1, Predicted - 1\n",
      "Test-62: Target - 0, Predicted - 0\n",
      "Test-63: Target - 0, Predicted - 0\n",
      "Test-64: Target - 0, Predicted - 0\n",
      "Test-65: Target - 1, Predicted - 0\n",
      "Test-66: Target - 1, Predicted - 1\n",
      "Test-67: Target - 0, Predicted - 0\n",
      "Test-68: Target - 1, Predicted - 0\n",
      "Test-69: Target - 0, Predicted - 0\n",
      "Test-70: Target - 1, Predicted - 1\n",
      "Test-71: Target - 0, Predicted - 0\n",
      "Test-72: Target - 0, Predicted - 0\n",
      "Test-73: Target - 0, Predicted - 0\n",
      "Test-74: Target - 0, Predicted - 0\n",
      "Test-75: Target - 1, Predicted - 0\n",
      "Test-76: Target - 0, Predicted - 0\n",
      "Test-77: Target - 1, Predicted - 1\n",
      "Test-78: Target - 0, Predicted - 1\n",
      "Test-79: Target - 0, Predicted - 0\n",
      "Test-80: Target - 0, Predicted - 0\n",
      "Test-81: Target - 0, Predicted - 0\n",
      "Test-82: Target - 0, Predicted - 0\n",
      "Test-83: Target - 1, Predicted - 1\n",
      "Test-84: Target - 1, Predicted - 1\n",
      "Test-85: Target - 0, Predicted - 0\n",
      "Test-86: Target - 0, Predicted - 0\n",
      "Test-87: Target - 0, Predicted - 0\n",
      "Test-88: Target - 0, Predicted - 1\n",
      "Test-89: Target - 1, Predicted - 1\n",
      "Test-90: Target - 0, Predicted - 0\n",
      "Test-91: Target - 0, Predicted - 0\n",
      "Test-92: Target - 0, Predicted - 0\n",
      "Test-93: Target - 1, Predicted - 1\n",
      "Test-94: Target - 0, Predicted - 0\n",
      "Test-95: Target - 0, Predicted - 0\n",
      "Test-96: Target - 0, Predicted - 0\n",
      "Test-97: Target - 0, Predicted - 0\n",
      "Test-98: Target - 0, Predicted - 0\n",
      "Test-99: Target - 1, Predicted - 1\n",
      "Test-100: Target - 0, Predicted - 0\n",
      "Test-101: Target - 0, Predicted - 0\n",
      "Test-102: Target - 0, Predicted - 0\n",
      "Test-103: Target - 0, Predicted - 0\n",
      "Test-104: Target - 0, Predicted - 0\n",
      "Test-105: Target - 0, Predicted - 0\n",
      "Test-106: Target - 0, Predicted - 0\n",
      "Test-107: Target - 0, Predicted - 0\n",
      "Test-108: Target - 1, Predicted - 1\n",
      "Test-109: Target - 0, Predicted - 0\n",
      "Test-110: Target - 0, Predicted - 0\n",
      "Test-111: Target - 0, Predicted - 1\n",
      "Test-112: Target - 0, Predicted - 0\n",
      "Test-113: Target - 0, Predicted - 0\n",
      "Test-114: Target - 0, Predicted - 1\n",
      "Test-115: Target - 1, Predicted - 0\n",
      "Test-116: Target - 0, Predicted - 0\n",
      "Test-117: Target - 0, Predicted - 0\n",
      "Test-118: Target - 0, Predicted - 0\n",
      "Test-119: Target - 1, Predicted - 1\n",
      "Test-120: Target - 0, Predicted - 0\n",
      "Test-121: Target - 1, Predicted - 0\n",
      "Test-122: Target - 0, Predicted - 0\n",
      "Test-123: Target - 1, Predicted - 1\n",
      "Test-124: Target - 0, Predicted - 1\n",
      "Test-125: Target - 0, Predicted - 0\n",
      "Test-126: Target - 0, Predicted - 1\n",
      "Test-127: Target - 0, Predicted - 0\n",
      "Test-128: Target - 0, Predicted - 0\n",
      "Test-129: Target - 0, Predicted - 0\n",
      "Test-130: Target - 0, Predicted - 0\n",
      "Test-131: Target - 0, Predicted - 0\n",
      "Test-132: Target - 0, Predicted - 0\n",
      "Test-133: Target - 0, Predicted - 0\n",
      "Test-134: Target - 0, Predicted - 0\n",
      "Test-135: Target - 0, Predicted - 0\n",
      "Test-136: Target - 0, Predicted - 0\n",
      "Test-137: Target - 1, Predicted - 1\n",
      "Test-138: Target - 0, Predicted - 1\n",
      "Test-139: Target - 1, Predicted - 1\n",
      "Test-140: Target - 0, Predicted - 1\n",
      "Test-141: Target - 0, Predicted - 0\n",
      "Test-142: Target - 0, Predicted - 0\n",
      "Test-143: Target - 0, Predicted - 0\n",
      "Test-144: Target - 1, Predicted - 1\n",
      "Test-145: Target - 0, Predicted - 0\n",
      "Test-146: Target - 0, Predicted - 1\n",
      "Test-147: Target - 1, Predicted - 1\n",
      "Test-148: Target - 0, Predicted - 1\n",
      "Test-149: Target - 1, Predicted - 1\n",
      "Test-150: Target - 0, Predicted - 0\n",
      "Test-151: Target - 0, Predicted - 1\n",
      "Test-152: Target - 0, Predicted - 0\n",
      "Test-153: Target - 0, Predicted - 0\n",
      "Test-154: Target - 0, Predicted - 1\n",
      "Test-155: Target - 0, Predicted - 0\n",
      "Test-156: Target - 0, Predicted - 0\n",
      "Test-157: Target - 0, Predicted - 1\n",
      "Test-158: Target - 0, Predicted - 1\n",
      "Test-159: Target - 0, Predicted - 0\n",
      "Test-160: Target - 0, Predicted - 1\n",
      "Test-161: Target - 0, Predicted - 1\n",
      "Test-162: Target - 1, Predicted - 1\n",
      "Test-163: Target - 0, Predicted - 0\n",
      "Test-164: Target - 0, Predicted - 0\n",
      "Test-165: Target - 0, Predicted - 0\n",
      "Test-166: Target - 1, Predicted - 1\n",
      "Test-167: Target - 0, Predicted - 0\n",
      "Test-168: Target - 0, Predicted - 0\n",
      "Test-169: Target - 1, Predicted - 1\n",
      "Test-170: Target - 0, Predicted - 1\n",
      "Test-171: Target - 0, Predicted - 1\n",
      "Test-172: Target - 1, Predicted - 1\n",
      "Test-173: Target - 0, Predicted - 0\n",
      "Test-174: Target - 1, Predicted - 1\n",
      "Test-175: Target - 0, Predicted - 0\n",
      "Test-176: Target - 1, Predicted - 1\n",
      "Test-177: Target - 0, Predicted - 0\n",
      "Test-178: Target - 0, Predicted - 1\n",
      "Test-179: Target - 0, Predicted - 0\n",
      "Test-180: Target - 0, Predicted - 0\n",
      "Test-181: Target - 0, Predicted - 0\n",
      "Test-182: Target - 0, Predicted - 0\n",
      "Test-183: Target - 0, Predicted - 0\n",
      "Test-184: Target - 0, Predicted - 1\n",
      "Test-185: Target - 0, Predicted - 0\n",
      "Test-186: Target - 0, Predicted - 0\n",
      "Test-187: Target - 0, Predicted - 0\n",
      "Test-188: Target - 0, Predicted - 0\n",
      "Test-189: Target - 0, Predicted - 0\n",
      "Test-190: Target - 1, Predicted - 1\n",
      "Test-191: Target - 0, Predicted - 0\n",
      "Test-192: Target - 1, Predicted - 1\n",
      "Test-193: Target - 0, Predicted - 0\n",
      "Test-194: Target - 1, Predicted - 1\n",
      "Test-195: Target - 0, Predicted - 0\n",
      "Test-196: Target - 0, Predicted - 0\n",
      "Test-197: Target - 1, Predicted - 1\n",
      "Test-198: Target - 0, Predicted - 0\n",
      "Test-199: Target - 0, Predicted - 0\n",
      "Test-200: Target - 0, Predicted - 0\n"
     ]
    }
   ],
   "source": [
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "false_positive = 0\n",
    "true_positive = 0\n",
    "\n",
    "#Repeat for each validation record\n",
    "for i in range(test_data.shape[0]):\n",
    "    \n",
    "    #Separate the taregt label (expected outcome)\n",
    "    sample_target = int(test_data.loc[i, 0])\n",
    "    \n",
    "    #Transform rest of the record into input feature set\n",
    "    sample_features = test_data.loc[i, 1:].to_frame().transpose().to_csv(index=False, header=False).split(',')\n",
    "    sample_request = json.dumps({\"instances\": [{\"features\": sample_features}]})\n",
    "    \n",
    "    #Invoke inference endpoint\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=sample_request\n",
    "    )\n",
    "    \n",
    "    #Compare target vs. predicted\n",
    "    predictions = json.loads(response['Body'].read().decode())['predictions']\n",
    "    sample_predicted = int(predictions[0]['predicted_label'])\n",
    "    print(\"Test-{}: Target - {}, Predicted - {}\".format(i+1, sample_target, sample_predicted))\n",
    "    \n",
    "    if sample_target == 0:\n",
    "        if sample_predicted == 0:\n",
    "            true_negative = true_negative + 1\n",
    "        else:\n",
    "            false_positive = false_positive + 1\n",
    "    else:\n",
    "        if sample_predicted == 1:\n",
    "            true_positive = true_positive + 1\n",
    "        else:\n",
    "            false_negative = false_negative + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "True Negative = 122\n",
      "False Positive = 32\n",
      "True Positive = 37\n",
      "False Negative = 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Results:\")\n",
    "print(\"True Negative = {}\".format(true_negative))\n",
    "print(\"False Positive = {}\".format(false_positive))\n",
    "print(\"True Positive = {}\".format(true_positive))\n",
    "print(\"False Negative = {}\".format(false_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With the counts of prediction outcome computed, we can use the simple formulas to determine the Precision, Recall and F1 score for the model.\n",
    "\n",
    "![Image](https://miro.medium.com/max/1520/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg)\n",
    "\n",
    "![Image](https://miro.medium.com/max/888/1*7J08ekAwupLBegeUI8muHA.png)\n",
    "\n",
    "\n",
    "\n",
    "![Image](https://miro.medium.com/max/564/1*T6kVUKxG_Z4V5Fm1UXhEIw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.54\n",
      "Recall : 0.80\n",
      "F1 Score : 0.64\n"
     ]
    }
   ],
   "source": [
    "precision = true_positive/(true_positive+false_positive)\n",
    "recall = true_positive/(true_positive+false_negative)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Precision : %.2f\" % precision)\n",
    "print(\"Recall : %.2f\" % recall)\n",
    "print(\"F1 Score : %.2f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Results\n",
    "\n",
    "Notice that, even without any Hyperparameter tuning, using the SageMaker Linear Learner, our first attempt yielded results comparable to that obtained by using Scikit Learn Logistic Regression classifier.\n",
    "\n",
    "|        \t| Scikit-Learn Logistic Regression | SageMaker Linear Learner |\n",
    "|----------\t|:-------------:\t|------:\t|\n",
    "| Precision |  0.63 \t| 0.54\t|\n",
    "| Recall \t|    0.55   \t|   0.80\t|\n",
    "| F1 Score \t| 0.59\t|    0.64 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "When a model is deployed behind endpoint using SageMaker's managed deployment environments, unlike local test environments, you'll end up having a persistent artifact, backed by an instance that will be running until terminated.\n",
    "\n",
    "Keep in mind that you by the usage for running instances. Therefore for any exercises, such as this one, that you conduct as proof of concept, it is very important to destroy the artifacts when not needed.\n",
    "\n",
    "`Boto3` API provides convenient methods to remove all such artefacts, thereby saving you on cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f7c75e7a-d2cf-44d8-9fba-4e7aa7a072cb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f7c75e7a-d2cf-44d8-9fba-4e7aa7a072cb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 02 Mar 2021 15:53:52 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.delete_endpoint(EndpointName = endpoint_name)\n",
    "sagemaker.delete_endpoint_config(EndpointConfigName = endpoint_config_name)\n",
    "sagemaker.delete_model(ModelName = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
